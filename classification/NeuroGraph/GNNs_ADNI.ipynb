{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import Adam\n",
    "from utils import *  # NeuroGraph\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function for generating multiple sets of arguments from one object ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function from one dictionary to multiple sets of params\n",
    "def grid_from_param(dic): #, append_model_name = True, model_name_default = 'default'):\n",
    "    # retrieve all lists to choose parameters from\n",
    "    names = []\n",
    "    lens = []\n",
    "    for name in dic:\n",
    "        item = dic[name]\n",
    "        if(type(item)==list):\n",
    "            names.append(name)\n",
    "            lens.append(len(item))\n",
    "\n",
    "    #helper\n",
    "    count = 1\n",
    "    mults = []\n",
    "    for l in lens:\n",
    "        mults.append(count)\n",
    "        count *= l\n",
    "    #print(mults)\n",
    "    #print(count)\n",
    "    #print(lens)\n",
    "\n",
    "    #construct the grid\n",
    "    params = []\n",
    "    n = len(lens)\n",
    "    for i in range(count):\n",
    "        param = dic.copy()\n",
    "        param['tune_name'] = '_'\n",
    "        for j in range(n):\n",
    "            param[names[j]] = dic[names[j]][i//mults[j] % lens[j]]\n",
    "            param['tune_name'] += f\"{names[j]}{param[names[j]]}_\"\n",
    "            # if param['tune_name'] contains '/', replace with '_'\n",
    "            param['tune_name'] = param['tune_name'].replace('/', '_')\n",
    "        params.append(param)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Args: # now it's just a wrapper for compatibility. Everything now packed up in the dictionary.\n",
    "    def __init__(self, param_dict) -> None:\n",
    "        # wrapped. see argDict above.\n",
    "        self.dataset = param_dict['dataset']\n",
    "        self.dataset_dir = param_dict['dataset_dir']\n",
    "        self.edge_dir_prefix = param_dict['edge_dir_prefix']\n",
    "        self.model = param_dict['model']\n",
    "        self.num_classes = param_dict['num_classes']\n",
    "        self.weight_decay = param_dict['weight_decay']\n",
    "        self.batch_size = param_dict['batch_size']\n",
    "        self.hidden_mlp = param_dict['hidden_mlp']\n",
    "        self.hidden = param_dict['hidden']\n",
    "        self.num_layers = param_dict['num_layers']\n",
    "        self.runs = param_dict['runs']\n",
    "        self.lr = param_dict['lr']\n",
    "        self.epochs = param_dict['epochs']\n",
    "        self.edge_percent_to_keep = param_dict['edge_percent_to_keep'] \n",
    "        self.seed = param_dict['seed']\n",
    "        self.n_splits = param_dict['n_splits'] if \"n_splits\" in param_dict else 5\n",
    "        self.device = \"cpu\" if self.model != \"GATConv\" else \"cpu\"\n",
    "        self.tune_name = param_dict['tune_name'] if \"tune_name\" in param_dict else None\n",
    "    def tuning_list(param_dicts : dict):\n",
    "        p = grid_from_param(param_dicts)\n",
    "        return [Args(x) for x in p]\n",
    "\n",
    "\n",
    "# #print([x['lr'] for x in grid_from_param(argsdict)])\n",
    "# args_list = Args.tuning_list(argsDictTune)\n",
    "# fix_seed(args_list[0].seed)\n",
    "\n",
    "# #print(len(Args.tuning_list(argsDictTune)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading our Datasets\n",
    "use our HCP correlation matrix dataset, train/test split file, label file.\n",
    "\n",
    "HCP data is downloaded from https://drive.google.com/drive/folders/166wCCtPOEL0O25FxzwB0I8AQA8b6Q9U1?usp=drive_link \n",
    "\n",
    "other files are in the data folder\n",
    "\n",
    "use our ADNI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_adni_data(args):\n",
    "    fMRI_path = args.dataset_dir + \"fmri_signal.mat\"\n",
    "    ICV_path = args.dataset_dir + \"ICV.mat\"\n",
    "    AGE_path = args.dataset_dir + \"AGE.mat\"\n",
    "    DX_path = args.dataset_dir + \"DX.mat\"\n",
    "    gender_path = args.dataset_dir + \"gender.mat\"\n",
    "    fMRI_data_path = args.dataset_dir + \"fMRIdata_ADNI2_ADNI3.csv\"\n",
    "    # participants_path = r'./data/ADNI/participants.tsv'\n",
    "\n",
    "    # read fMRI_path\n",
    "    fmri_data = scipy.io.loadmat(fMRI_path)['fmri_signal']\n",
    "    fMRI_data = [fmri_data[i][0] for i in range(len(fmri_data))]\n",
    "\n",
    "    # read ICV_path\n",
    "    icv_data = scipy.io.loadmat(ICV_path)['ICV']\n",
    "    ICV_data = pd.DataFrame([icv_data[i][0] for i in range(len(icv_data))])\n",
    "\n",
    "    # read AGE_path\n",
    "    age_data = scipy.io.loadmat(AGE_path)['AGE']\n",
    "    AGE_data = pd.DataFrame([age_data[i][0] for i in range(len(age_data))])\n",
    "\n",
    "    # read gender_path\n",
    "    gender_data = scipy.io.loadmat(gender_path)['gender']\n",
    "    gender_data = pd.DataFrame([gender_data[i][0] for i in range(len(gender_data))])\n",
    "\n",
    "    # read DX_path\n",
    "    dx_data = scipy.io.loadmat(DX_path)['DX']\n",
    "    DX_data = pd.DataFrame([dx_data[i][0] for i in range(len(dx_data))])\n",
    "\n",
    "    # for all above variable, add a df.insert(0, 'Image_ID', range(1, 1 + len(fMRI_data))) to add Image_ID column\n",
    "    for df in [ICV_data, AGE_data, gender_data, DX_data]:\n",
    "        df.insert(0, 'Image_ID', range(1, 1 + len(fMRI_data)))\n",
    "\n",
    "    # give their column names, EstimatedTotalIntraCranialVol, Age, Gender, Diagnosis\n",
    "    ICV_data.columns = ['Image_ID', 'EstimatedTotalIntraCranialVol']\n",
    "    AGE_data.columns = ['Image_ID', 'Age']\n",
    "    gender_data.columns = ['Image_ID', 'Gender']\n",
    "    DX_data.columns = ['Image_ID', 'Diagnosis']\n",
    "    Image_ID = ICV_data['Image_ID']\n",
    "\n",
    "    data_dict = {\n",
    "        'fMRI_data': fMRI_data,\n",
    "        'ICV_data': ICV_data,\n",
    "        'AGE_data': AGE_data,\n",
    "        'gender_data': gender_data,\n",
    "        'DX_data': DX_data\n",
    "    }\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_args(args: Args):\n",
    "\n",
    "    # Label path\n",
    "    labels_file = args.dataset_dir + 'y.csv'\n",
    "    # Load labels\n",
    "    labels_df = pd.read_csv(labels_file)\n",
    "\n",
    "    # for ADNI Dataset\n",
    "    if args.dataset == \"ADNI\":\n",
    "        adni_data = read_adni_data(args)\n",
    "        fMRI_data = adni_data['fMRI_data']\n",
    "        ICV_data = adni_data['ICV_data']\n",
    "        AGE_data = adni_data['AGE_data']\n",
    "        gender_data = adni_data['gender_data']\n",
    "        DX_data = adni_data['DX_data']\n",
    "\n",
    "        # only keep healthy control and AD. namely 2 and 0\n",
    "        labels_df = labels_df[labels_df['Diagnosis'].isin([2, 0])].reset_index(drop=True)\n",
    "        # change all 2 to 1\n",
    "        labels_df['Diagnosis'] = labels_df['Diagnosis'].replace({2: 1})\n",
    "\n",
    "        dataset = []\n",
    "        # traverse the labels_df by i\n",
    "        for i in range(len(labels_df)):\n",
    "            IID = labels_df['IID'][i]\n",
    "            y = labels_df['Diagnosis'][i]\n",
    "            # turn y to <class 'torch.Tensor'>\n",
    "            y = torch.tensor(y, dtype=torch.long)\n",
    "            # z-score normalization for each column of each subject\n",
    "            subject_data = fMRI_data[IID]\n",
    "            # fill 0 with 1\n",
    "            subject_data[subject_data == 0] = 1\n",
    "            subject_data = (subject_data - np.mean(subject_data, axis=0)) / np.std(subject_data, axis=0)\n",
    "\n",
    "            try:\n",
    "                edge_attr = pd.read_csv(args.dataset_dir + 'fmri_edge/' + args.edge_dir_prefix + str(IID) + '.csv')\n",
    "            except:\n",
    "                print('File \\\"' + args.dataset_dir + 'fmri_edge/' + args.edge_dir_prefix + str(IID) + '.csv\\\" not found. Skipping.')\n",
    "                continue\n",
    "            edge_attr = edge_attr.to_numpy()\n",
    "            np.fill_diagonal(edge_attr, 0)\n",
    "\n",
    "            # get the threshold of edge_attr\n",
    "            threshold = np.percentile(edge_attr, 100 * (1 - args.edge_percent_to_keep))\n",
    "\n",
    "            # only keep edges that are larger than the threshold\n",
    "            edge_attr[edge_attr <= threshold] = 0\n",
    "\n",
    "            total_edge_count = edge_attr.shape[0] * edge_attr.shape[1]\n",
    "            target_edge_count = int(args.edge_percent_to_keep * total_edge_count)\n",
    "            edge_index = np.vstack(np.nonzero(edge_attr))\n",
    "\n",
    "            filtered_edge_attr = edge_attr[edge_index[0], edge_index[1]]\n",
    "            filtered_edge_attr = torch.tensor(filtered_edge_attr, dtype=torch.float)\n",
    "            current_edge_count = filtered_edge_attr.shape[0]\n",
    "\n",
    "            # Adjust the number of edges without randomness\n",
    "            # Adjust the number of edges without randomness\n",
    "            if current_edge_count > target_edge_count:\n",
    "                # Sort edges by their weights in descending order and keep the top edges\n",
    "                sorted_indices = torch.argsort(filtered_edge_attr, descending=True)\n",
    "                indices_to_keep = sorted_indices[:target_edge_count]\n",
    "                edge_index = edge_index[:, indices_to_keep]\n",
    "                filtered_edge_attr = filtered_edge_attr[indices_to_keep]\n",
    "            elif current_edge_count < target_edge_count:\n",
    "                # Sort edges by their weights in ascending order and add the smallest edges until the target is met\n",
    "                sorted_indices = torch.argsort(filtered_edge_attr, descending=False)\n",
    "                indices_to_add = sorted_indices[:target_edge_count - current_edge_count]\n",
    "\n",
    "                # Convert indices_to_add to a NumPy array\n",
    "                indices_to_add = indices_to_add.cpu().numpy()\n",
    "\n",
    "                # Index edge_index with indices_to_add\n",
    "                edge_index_to_add = edge_index[:, indices_to_add]\n",
    "\n",
    "                # Ensure edge_index_to_add has the correct shape\n",
    "                if edge_index_to_add.ndim == 1:\n",
    "                    edge_index_to_add = edge_index_to_add.reshape(2, 1)\n",
    "\n",
    "                # Stack the edge indices\n",
    "                edge_index = np.hstack([edge_index, edge_index_to_add])\n",
    "\n",
    "                # Similarly, handle filtered_edge_attr\n",
    "                filtered_edge_attr_to_add = filtered_edge_attr[indices_to_add]\n",
    "                if filtered_edge_attr_to_add.ndim == 0:\n",
    "                    filtered_edge_attr_to_add = filtered_edge_attr_to_add.unsqueeze(0)\n",
    "\n",
    "                filtered_edge_attr = torch.cat([filtered_edge_attr, filtered_edge_attr_to_add])\n",
    "\n",
    "\n",
    "            # Create the Data object\n",
    "            data = Data(x=torch.tensor(edge_attr, dtype=torch.float), \n",
    "                        edge_index=torch.tensor(edge_index, dtype=torch.long), \n",
    "                        edge_attr=filtered_edge_attr, \n",
    "                        y=y)\n",
    "\n",
    "            # Append the processed data\n",
    "            dataset.append(data)\n",
    "        return dataset\n",
    "\n",
    "    # for HCP\n",
    "    elif args.dataset == \"HCP\":\n",
    "        path = \"/home/songlinzhao/task-driven-parcellation/baseline/data/HCP/HCPGender.pt\"\n",
    "        # Load the data\n",
    "        data = torch.load(path)\n",
    "        pass\n",
    "    \n",
    "\n",
    "\n",
    "    elif args.dataset == \"ADHD\":\n",
    "        labels_file = args.dataset_dir + 'y.csv'\n",
    "        # Load labels\n",
    "        labels_df = pd.read_csv(labels_file)\n",
    "\n",
    "        # Assuming Diagnosis column exists for ADHD labels\n",
    "        # Filter data to include only specific classes (e.g., HC and ADHD)\n",
    "        labels_df = labels_df[labels_df['Diagnosis'].isin([2, 0])].reset_index(drop=True)\n",
    "\n",
    "        # Change labels for consistency (e.g., HC -> 1, ADHD -> 0)\n",
    "        labels_df['Diagnosis'] = labels_df['Diagnosis'].replace({2: 1, 0: 0})\n",
    "\n",
    "        dataset = []\n",
    "        all_edge_counts = []\n",
    "\n",
    "        # First pass to calculate max edge count\n",
    "        for i in range(len(labels_df)):\n",
    "            IID = labels_df['IID'][i]\n",
    "            try:\n",
    "                edge_attr = pd.read_csv(args.dataset_dir + 'fmri_edge/' + args.edge_dir_prefix + str(IID) + '.csv')\n",
    "            except:\n",
    "                print(f'File \\\"{args.dataset_dir}fmri_edge/{args.edge_dir_prefix}{IID}.csv\\\" not found. Skipping.')\n",
    "                continue\n",
    "\n",
    "            edge_attr = edge_attr.to_numpy()\n",
    "            np.fill_diagonal(edge_attr, 0)\n",
    "            edge_index = np.vstack(np.nonzero(edge_attr))\n",
    "            all_edge_counts.append(edge_index.shape[1])  # Count edges in this graph\n",
    "\n",
    "        # Determine fixed edge count (e.g., use the maximum edge count or specify a fixed value)\n",
    "        fixed_edge_count = min(all_edge_counts)  # Choose minimum to ensure all graphs can match\n",
    "\n",
    "        # Second pass to create Data objects\n",
    "        for i in range(len(labels_df)):\n",
    "            IID = labels_df['IID'][i]\n",
    "            y = labels_df['Diagnosis'][i]\n",
    "            y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "            try:\n",
    "                edge_attr = pd.read_csv(args.dataset_dir + 'fmri_edge/' + args.edge_dir_prefix + str(IID) + '.csv')\n",
    "            except:\n",
    "                print(f'File \\\"{args.dataset_dir}fmri_edge/{args.edge_dir_prefix}{IID}.csv\\\" not found. Skipping.')\n",
    "                continue\n",
    "\n",
    "            edge_attr = edge_attr.to_numpy()\n",
    "            np.fill_diagonal(edge_attr, 0)\n",
    "\n",
    "            # Apply thresholding as in ADNI\n",
    "            threshold = np.percentile(edge_attr, 100 * (1 - args.edge_percent_to_keep))\n",
    "            edge_attr[edge_attr <= threshold] = 0\n",
    "\n",
    "            # Compute edge indices and attributes\n",
    "            edge_index = np.vstack(np.nonzero(edge_attr))\n",
    "            filtered_edge_attr = edge_attr[edge_index[0], edge_index[1]]\n",
    "            filtered_edge_attr = torch.tensor(filtered_edge_attr, dtype=torch.float)\n",
    "\n",
    "            # Adjust edge count to match fixed_edge_count\n",
    "            current_edge_count = edge_index.shape[1]\n",
    "            if current_edge_count > fixed_edge_count:\n",
    "                # Keep top-weighted edges\n",
    "                sorted_indices = torch.argsort(filtered_edge_attr, descending=True)\n",
    "                indices_to_keep = sorted_indices[:fixed_edge_count]\n",
    "                edge_index = edge_index[:, indices_to_keep]\n",
    "                filtered_edge_attr = filtered_edge_attr[indices_to_keep]\n",
    "            elif current_edge_count < fixed_edge_count:\n",
    "                # Add lowest-weighted edges until fixed_edge_count is met\n",
    "                sorted_indices = torch.argsort(filtered_edge_attr, descending=False)\n",
    "                indices_to_add = sorted_indices[:fixed_edge_count - current_edge_count]\n",
    "                edge_index_to_add = edge_index[:, indices_to_add]\n",
    "                filtered_edge_attr_to_add = filtered_edge_attr[indices_to_add]\n",
    "                edge_index = np.hstack([edge_index, edge_index_to_add])\n",
    "                filtered_edge_attr = torch.cat([filtered_edge_attr, filtered_edge_attr_to_add])\n",
    "\n",
    "            # Create Data object\n",
    "            data = Data(x=torch.tensor(edge_attr, dtype=torch.float),\n",
    "                        edge_index=torch.tensor(edge_index, dtype=torch.long),\n",
    "                        edge_attr=filtered_edge_attr,\n",
    "                        y=y)\n",
    "\n",
    "            dataset.append(data)\n",
    "\n",
    "        return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Functions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, args: Args, train_loader):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:  \n",
    "        data = data.to(args.device)\n",
    "        # print('data:', data)\n",
    "        out = model(data) \n",
    "        loss = criterion(out, data.y) \n",
    "        total_loss +=loss\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        optimizer.zero_grad()\n",
    "    return total_loss/len(train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, args: Args, loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(args.device)\n",
    "            out = model(data)\n",
    "            probs = F.softmax(out, dim=1)  # Calculate probabilities\n",
    "            preds = out.argmax(dim=1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy()[:, 1])  # Keep the probabilities of the positive class\n",
    "            all_labels.append(data.y.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_probs = np.concatenate(all_probs)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    auroc = roc_auc_score(all_labels, all_probs)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds).ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'auroc': auroc,\n",
    "        'sensitivity': sensitivity,\n",
    "        'specificity': specificity,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# test for multiclass\n",
    "def test_multiclass(model, args: Args, loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(args.device)\n",
    "            out = model(data)\n",
    "            probs = F.softmax(out, dim=1)  # calculate probabilities for each class\n",
    "            preds = out.argmax(dim=1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "            all_labels.append(data.y.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_probs = np.concatenate(all_probs, axis=0)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    \n",
    "    # metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    # here is how to calculate auroc for multiclass\n",
    "    auroc = roc_auc_score(all_labels, all_probs, multi_class='ovr')  # ovr should be used for multiclass\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')  # weighted should be used for multiclass\n",
    "    # confusion_matrix need to be calculated for each class\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    # sensitivity and specificity for each class\n",
    "    sensitivities = []\n",
    "    specificities = []\n",
    "    for i in range(cm.shape[0]):\n",
    "        tp = cm[i, i]\n",
    "        fn = cm[i, :].sum() - tp\n",
    "        fp = cm[:, i].sum() - tp\n",
    "        tn = cm.sum() - (tp + fn + fp)\n",
    "        sensitivity = tp / (tp + fn)\n",
    "        specificity = tn / (tn + fp)\n",
    "        sensitivities.append(sensitivity)\n",
    "        specificities.append(specificity)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'auroc': auroc,\n",
    "        'sensitivity': np.mean(sensitivities),\n",
    "        'specificity': np.mean(specificities),\n",
    "        'f1_score': f1\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def bench_from_args(args: Args, verbose = False):\n",
    "    # get train and test data\n",
    "    # train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "    # train_data, test_data = load_data_from_args(args)\n",
    "    full_data = load_data_from_args(args)\n",
    "\n",
    "    # print('full_data:', full_data)\n",
    "    # print('full_data:', full_data)\n",
    "    # Initialize KFold\n",
    "    kf = KFold(n_splits=args.n_splits, shuffle=True, random_state=args.seed)\n",
    "    \n",
    "    fold_metrics = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(full_data)):\n",
    "        print(f\"Fold {fold + 1}/{args.n_splits}\")\n",
    "\n",
    "        # Create train and validation data loaders for this fold\n",
    "        train_data = [full_data[i] for i in train_idx]\n",
    "        test_data = [full_data[i] for i in test_idx]\n",
    "\n",
    "        train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=args.seed)\n",
    "\n",
    "        # create data loaders\n",
    "        train_loader = DataLoader(train_data, args.batch_size, shuffle=True, drop_last=True)\n",
    "        val_loader = DataLoader(val_data, args.batch_size, shuffle=False, drop_last=False)\n",
    "        test_loader = DataLoader(test_data, args.batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "        checkpoints_dir = './checkpoints/'\n",
    "        if not os.path.exists(checkpoints_dir):\n",
    "            os.makedirs(checkpoints_dir)\n",
    "\n",
    "        val_acc_history, test_acc_history, test_loss_history = [],[],[]\n",
    "        #seed = 42\n",
    "        for index in range(args.runs):\n",
    "            gnn = eval(args.model)\n",
    "            model = ResidualGNNs(args, train_data, args.hidden, args.hidden_mlp, args.num_layers, gnn).to(args.device) ## apply GNN*\n",
    "            if (verbose):\n",
    "                print(model)\n",
    "            #total_params = sum(p.numel() for p in model.parameters())\n",
    "            loss, test_acc = [], []\n",
    "            best_val_auroc, best_val_loss = 0.0,0.0\n",
    "            for epoch in tqdm(range(args.epochs), desc=\"Training Epochs\"):\n",
    "                loss = train(model, args, train_loader)\n",
    "                val_metrics = test(model, args, val_loader)\n",
    "                \n",
    "                if verbose:\n",
    "                    train_metrics = test(model, args, train_loader)\n",
    "                    test_metrics = test(model, args, test_loader)\n",
    "                    print(\"epoch: {}, loss: {}, \\ntrain_metrics:{}, \\nval_metrics:{}, \\ntest_metrics:{}\".format(\n",
    "                        epoch, np.round(loss.item(), 6), train_metrics, val_metrics, test_metrics))\n",
    "                \n",
    "                # 检查是否是最好的验证 AUROC\n",
    "                if val_metrics['auroc'] > best_val_auroc:\n",
    "                    best_val_auroc = val_metrics['auroc']\n",
    "                    torch.save(model.state_dict(), f\"{checkpoints_dir}{args.dataset}_{args.edge_dir_prefix.split('/')[0]}_{args.model}{args.tune_name}task-checkpoint-best-auroc.pkl\")\n",
    "        #test the model\n",
    "        model.load_state_dict(torch.load(f\"{checkpoints_dir}{args.dataset}_{args.edge_dir_prefix.split('/')[0]}_{args.model}{args.tune_name}task-checkpoint-best-auroc.pkl\"))\n",
    "        model.eval()\n",
    "        test_metrics = test(model, args, test_loader)\n",
    "        fold_metrics.append(val_metrics)\n",
    "        if (verbose):\n",
    "            print(f\"Fold {fold + 1} Test Metrics: {val_metrics}\")\n",
    "\n",
    "        if (verbose):\n",
    "            print('test_metrics:', test_metrics)\n",
    "    # Aggregate results\n",
    "    avg_metrics = {key: np.mean([fold[key] for fold in fold_metrics]) for key in fold_metrics[0].keys()}\n",
    "\n",
    "    # get results std of each metric on all folds\n",
    "    std_metrics = {key: np.std([fold[key] for fold in fold_metrics]) for key in fold_metrics[0].keys()}\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Average Metrics: {avg_metrics}\")\n",
    "        print(f\"Std Metrics: {std_metrics}\")\n",
    "    \n",
    "    \n",
    "    return avg_metrics, std_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### demo of full usage ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_edge_dir_prefixpearson_correlation_pearson_correlation_epochs20_edge_percent_to_keep0.2_\n",
      "{'accuracy': 0.6285714285714287, 'auroc': 0.6415017277685944, 'sensitivity': 0.6841312506140091, 'specificity': 0.5182113738452412, 'f1_score': 0.7119737919124363}\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:14<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_edge_dir_prefixcosine_similarity_cosine_similarity_epochs20_edge_percent_to_keep0.2_\n",
      "{'accuracy': 0.6493506493506495, 'auroc': 0.6571909821067873, 'sensitivity': 0.7167389723941449, 'specificity': 0.5222293414696161, 'f1_score': 0.7320461020461021}\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:09<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:10<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:09<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:09<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:09<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_edge_dir_prefixknn_graph_knn_graph_epochs20_edge_percent_to_keep0.2_\n",
      "{'accuracy': 0.5662337662337662, 'auroc': 0.7035078758416564, 'sensitivity': 0.4704082915807054, 'specificity': 0.7895482668022714, 'f1_score': 0.5970778818248389}\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:12<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:12<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_edge_dir_prefixeuclidean_distance_euclidean_distance_epochs20_edge_percent_to_keep0.2_\n",
      "{'accuracy': 0.6857142857142857, 'auroc': 0.6427027840106038, 'sensitivity': 0.863580901856764, 'specificity': 0.27929824561403505, 'f1_score': 0.789208206646006}\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_edge_dir_prefixspearman_correlation_spearman_correlation_epochs20_edge_percent_to_keep0.2_\n",
      "{'accuracy': 0.6207792207792207, 'auroc': 0.6667876848800072, 'sensitivity': 0.6373115237253169, 'specificity': 0.599025680142385, 'f1_score': 0.6963521123441956}\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_edge_dir_prefixkendall_correlation_kendall_correlation_epochs20_edge_percent_to_keep0.2_\n",
      "{'accuracy': 0.7142857142857143, 'auroc': 0.7197135377430494, 'sensitivity': 0.7871934374692996, 'specificity': 0.546343927451479, 'f1_score': 0.7870867133852706}\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_edge_dir_prefixpartial_correlation_partial_correlation_epochs20_edge_percent_to_keep0.2_\n",
      "{'accuracy': 0.574025974025974, 'auroc': 0.6349258245371242, 'sensitivity': 0.6337844581982512, 'specificity': 0.49112907873548606, 'f1_score': 0.6098353305751066}\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_edge_dir_prefixcross_correlation_cross_correlation_epochs20_edge_percent_to_keep0.2_\n",
      "{'accuracy': 0.6103896103896104, 'auroc': 0.6438227557386399, 'sensitivity': 0.6547969348659004, 'specificity': 0.5072070514450376, 'f1_score': 0.6939987380774771}\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_edge_dir_prefixcorrelations_correlation_correlations_correlation_epochs20_edge_percent_to_keep0.2_\n",
      "{'accuracy': 0.5818181818181818, 'auroc': 0.6202526204284272, 'sensitivity': 0.58989566755084, 'specificity': 0.5668541401813713, 'f1_score': 0.6613362193362194}\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:14<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_edge_dir_prefixassociated_high_order_fc_associated_high_order_fc_epochs20_edge_percent_to_keep0.2_\n",
      "{'accuracy': 0.587012987012987, 'auroc': 0.5405639694772496, 'sensitivity': 0.6877125454366834, 'specificity': 0.36542147639630473, 'f1_score': 0.6930211399061775}\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_edge_dir_prefixmutual_information_mutual_information_epochs20_edge_percent_to_keep0.2_\n",
      "{'accuracy': 0.67012987012987, 'auroc': 0.6929814665316915, 'sensitivity': 0.7495076137145104, 'specificity': 0.503599966098822, 'f1_score': 0.7555907532072454}\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:13<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_edge_dir_prefixpatels_conditional_dependence_measures_kappa_patels_conditional_dependence_measures_kappa_epochs20_edge_percent_to_keep0.2_\n",
      "{'accuracy': 0.6233766233766234, 'auroc': 0.65138394815188, 'sensitivity': 0.6713698791629825, 'specificity': 0.5194969065175015, 'f1_score': 0.700237908247617}\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:12<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:12<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:12<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:12<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 20/20 [00:12<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_edge_dir_prefixpatels_conditional_dependence_measures_tau_patels_conditional_dependence_measures_tau_epochs20_edge_percent_to_keep0.2_\n",
      "{'accuracy': 0.6337662337662338, 'auroc': 0.589849034547054, 'sensitivity': 0.7595844385499558, 'specificity': 0.3507971862022205, 'f1_score': 0.7396381149285338}\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:08<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:08<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:11<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:11<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:11<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_edge_dir_prefixpearson_correlation_pearson_correlation_epochs200_edge_percent_to_keep0.2_\n",
      "{'accuracy': 0.6233766233766234, 'auroc': 0.5791699809114786, 'sensitivity': 0.7242025739267118, 'specificity': 0.40712195948809216, 'f1_score': 0.7254258798572499}\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:12<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:12<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:12<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:11<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:09<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_edge_dir_prefixcosine_similarity_cosine_similarity_epochs200_edge_percent_to_keep0.2_\n",
      "{'accuracy': 0.6259740259740261, 'auroc': 0.586927415734328, 'sensitivity': 0.7206983004224383, 'specificity': 0.4236114924993643, 'f1_score': 0.7255639593894039}\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [01:38<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [01:40<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [01:39<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [01:40<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [01:40<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_edge_dir_prefixknn_graph_knn_graph_epochs200_edge_percent_to_keep0.2_\n",
      "{'accuracy': 0.6519480519480519, 'auroc': 0.6720309662982275, 'sensitivity': 0.7345360055015228, 'specificity': 0.47632782439189764, 'f1_score': 0.7437615146991157}\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:10<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:10<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:10<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:11<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:12<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_edge_dir_prefixeuclidean_distance_euclidean_distance_epochs200_edge_percent_to_keep0.2_\n",
      "{'accuracy': 0.6415584415584416, 'auroc': 0.6586562202479507, 'sensitivity': 0.6812826407309166, 'specificity': 0.5381723874904653, 'f1_score': 0.7193213363189057}\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:10<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:14<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:14<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:09<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:09<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_edge_dir_prefixspearman_correlation_spearman_correlation_epochs200_edge_percent_to_keep0.2_\n",
      "{'accuracy': 0.6831168831168831, 'auroc': 0.6697387352207838, 'sensitivity': 0.7901324295117398, 'specificity': 0.443101279769472, 'f1_score': 0.7745784137409754}\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:11<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:10<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:08<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:12<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:11<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_edge_dir_prefixkendall_correlation_kendall_correlation_epochs200_edge_percent_to_keep0.2_\n",
      "{'accuracy': 0.6831168831168831, 'auroc': 0.6796045148815606, 'sensitivity': 0.7896443658512624, 'specificity': 0.4475586066615815, 'f1_score': 0.7757794196353874}\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:08<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:10<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:09<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:07<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:09<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_edge_dir_prefixpartial_correlation_partial_correlation_epochs200_edge_percent_to_keep0.2_\n",
      "{'accuracy': 0.6285714285714286, 'auroc': 0.5498711069566431, 'sensitivity': 0.7898542096473131, 'specificity': 0.26826815831850154, 'f1_score': 0.7444159544159545}\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:12<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:12<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:11<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:13<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:13<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_edge_dir_prefixcross_correlation_cross_correlation_epochs200_edge_percent_to_keep0.2_\n",
      "{'accuracy': 0.612987012987013, 'auroc': 0.5909804629848818, 'sensitivity': 0.7392757638274879, 'specificity': 0.34652428171878974, 'f1_score': 0.7241425849175597}\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:15<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:14<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:15<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:12<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:14<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_edge_dir_prefixcorrelations_correlation_correlations_correlation_epochs200_edge_percent_to_keep0.2_\n",
      "{'accuracy': 0.631168831168831, 'auroc': 0.6034899271012268, 'sensitivity': 0.7266195107574418, 'specificity': 0.426291041613696, 'f1_score': 0.7313862888109652}\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:15<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:17<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:15<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:15<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:15<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_edge_dir_prefixassociated_high_order_fc_associated_high_order_fc_epochs200_edge_percent_to_keep0.2_\n",
      "{'accuracy': 0.5948051948051948, 'auroc': 0.5388467421709748, 'sensitivity': 0.7127476176441693, 'specificity': 0.34093194338503263, 'f1_score': 0.7074065501788274}\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:10<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:11<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:10<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:09<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:09<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_edge_dir_prefixmutual_information_mutual_information_epochs200_edge_percent_to_keep0.2_\n",
      "{'accuracy': 0.683116883116883, 'auroc': 0.7202520480650364, 'sensitivity': 0.8157828863346104, 'specificity': 0.40461123824052886, 'f1_score': 0.779973847960542}\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:10<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:12<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:12<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:11<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:12<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_edge_dir_prefixpatels_conditional_dependence_measures_kappa_patels_conditional_dependence_measures_kappa_epochs200_edge_percent_to_keep0.2_\n",
      "{'accuracy': 0.6779220779220779, 'auroc': 0.6601584736196904, 'sensitivity': 0.7843330386089008, 'specificity': 0.4362593440122044, 'f1_score': 0.7695258570300603}\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:12<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:12<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:13<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:11<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 200/200 [02:07<00:00,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_edge_dir_prefixpatels_conditional_dependence_measures_tau_patels_conditional_dependence_measures_tau_epochs200_edge_percent_to_keep0.2_\n",
      "{'accuracy': 0.6051948051948052, 'auroc': 0.5690139979492935, 'sensitivity': 0.7105338441890167, 'specificity': 0.3961901856089499, 'f1_score': 0.7114750784757419}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "argsDictTune_a = {\n",
    "    # choose dataset form: ADNI(BOLD), HCP(CORR), BOLD+CORR\n",
    "    'dataset' : \"ADHD\",\n",
    "    # data path\n",
    "    'dataset_dir' : \"../../data/ADHD/\", # ========================================= locally changed? =========================================\n",
    "    # choose from: GCNConv, GINConv, SGConv, GeneralConv, GATConv\n",
    "    'edge_dir_prefix' : [\n",
    "        'pearson_correlation/pearson_correlation',\n",
    "        \"cosine_similarity/cosine_similarity\",\n",
    "        # \"KNN_Graph/knn_graph_\",\n",
    "        \"knn_graph/knn_graph\",\n",
    "        # \"Euclidean_Distance/distance_matrix_\",\n",
    "        \"euclidean_distance/euclidean_distance\",\n",
    "        \"spearman_correlation/spearman_correlation\",\n",
    "        \"kendall_correlation/kendall_correlation\",\n",
    "        \"partial_correlation/partial_correlation\",\n",
    "        \"cross_correlation/cross_correlation\",        \n",
    "        # \"pairwise_PC_aHOFC/aHOFC\",\n",
    "        # \"pairwise_PC_dHOFC/dHOFC\",\n",
    "        # \"pairwise_PC_tHOFC/tHOFC\",\n",
    "        \"correlations_correlation/correlations_correlation\",\n",
    "        \"associated_high_order_fc/associated_high_order_fc\",\n",
    "\n",
    "        \n",
    "        \"mutual_information/mutual_information\",\n",
    "        # \"granger_causality/granger_causality\",\n",
    "        # #added——————————————————\n",
    "        # # \"CityblockDistance/distance_matrix_\",\n",
    "        # # \"DTWDistance/DTW_distance_\",\n",
    "        # \"EMDDistance/EMDdistance_matrix_\",\n",
    "        # \"WaveletCoherence/coherence_matrix_\",\n",
    "        # \"coherence_matrix/coherence_matrix\",\n",
    "        # # \"combined_correlation/combined_correlation\",\n",
    "        # \"lingam/lingam\",\n",
    "        # \"generalised_synchronisation_matrix/generalised_synchronisation_matrix\",\n",
    "        \"patels_conditional_dependence_measures_kappa/patels_conditional_dependence_measures_kappa\",\n",
    "        \"patels_conditional_dependence_measures_tau/patels_conditional_dependence_measures_tau\",\n",
    "\n",
    "        # \"PLV/MatrixAverage_\",\n",
    "        # \"PSI/PSI_matrix_\",\n",
    "        # \"sparse_inverse_covariance/sparse_inverse_covariance\",\n",
    "        # \"multi_region_SLR/SLR\",\n",
    "        # \"multi_region_SR/SR\",\n",
    "\n",
    "\n",
    "\n",
    "    ],\n",
    "    'model' : \"GCNConv\" ,\n",
    "    'num_classes' : 2,  # ADNI - binary classification\n",
    "    'weight_decay' : 0.0005,\n",
    "    # 'batch_size' : [16, 32],\n",
    "    'batch_size': 8,\n",
    "    'hidden_mlp' : 64,\n",
    "    'hidden' : 32,\n",
    "    # 'num_layers' : [2, 3, 4],\n",
    "    'num_layers': 5,\n",
    "    'runs' : 1,\n",
    "    # 'lr' : [1e-3, 1e-4, 1e-5],\n",
    "    'lr': 1e-3,\n",
    "    'epochs' : [20, 200],\n",
    "    'edge_percent_to_keep' : [0.2],\n",
    "    'n_splits' : 5,\n",
    "    'seed' : 42,\n",
    "    # 'verbose' : True\n",
    "}\n",
    "#print([x['lr'] for x in grid_from_param(argsdict)])\n",
    "args_list_a = Args.tuning_list(argsDictTune_a)\n",
    "fix_seed(args_list_a[0].seed)\n",
    "test_metric_list_a = []\n",
    "\n",
    "for args in args_list_a:\n",
    "    met, std = bench_from_args(args, verbose=False)\n",
    "    test_metric_list_a.append(met)\n",
    "    print(args.tune_name)\n",
    "    print(met)\n",
    "    # with open(\"result_{args.dataset}.txt\", \"a\") as f:\n",
    "    with open(f\"result_{args.dataset}.txt\", \"a\") as f:\n",
    "        f.write(f\"{args.tune_name}\\n{met}\\n--------------------------------\\n\")\n",
    "\n",
    "# for args in args_list_a:\n",
    "#     met = bench_from_args(args, verbose=False)\n",
    "#     test_metric_list_a.append(met)\n",
    "#     print(args.tune_name)\n",
    "#     print(met)\n",
    "#     # with open(\"result_{args.dataset}.txt\", \"a\") as f:\n",
    "#     with open(f\"result_{args.dataset}.txt\", \"a\") as f:\n",
    "#         f.write(f\"{args.tune_name}\\n{met}\\n--------------------------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
